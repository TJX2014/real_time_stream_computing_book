## 离线分析

离线数据处理和分析在实时流系统中也是非常重要的一部分。
在Lambda架构中，我们就已经看到了批处理系统对实时系统的辅助作用。
而离线数据处理和分析虽然并不会直接影响实时流系统的执行，
但是，离线系统对实时系统也有着很多的辅助作用。这些作用包括：

1. 数据落地和ETL处理
2. 离线数据分析和模型训练
3. 离线报表统计

针对这些离线任务，它们都有一个共同的模式。
数据需要存储下来，然后在这些数据基础上，做各种数据处理和分析。
针对此类任务，以Hadoop为基础的大数据生态为我们提供了非常好的解决方案。
围绕Hadoop的一些列软件和相关资源都非常丰富，因此本书不深入展开，感兴趣的读者可以自行上网查阅相关内容。
在此我们将重点关注离线任务的三个方面。

### 存储
实时流数据经过处理和分析后，需要进行数据落地。也就是将数据存入持久化存储设备里。
为了将实时流处理和数据落地的逻辑分离开来，我们最好是先将实时流数据发送到Kafka消息队列里，
然后从Kafka消息队列里拉取数据，最后将数据写入HDFS（Hadoop分布式文件系统）。
从Kafka里拉取消息写入HDFS的方法有很多种，比如使用Flume就是一种常用的方案。

### 处理和分析
围绕Hadoop有关数据处理和分析的工具有很多种，比较典型的如有Hive和Spark等。

#### Hive
Hive是一个数据仓库工具，它将结构化数据文件映射为数据库表，并提供SQL查询功能。
Hive内部将SQL语句转换为MapReduce或Tez作业，然后提交Hadoop执行。
因此可以将Hive理解为MapReduce或Tez的一层SQL皮肤。
使用Hive的好处正在于其对SQL的支持，只要有SQL的基础，就可以快速开始离线数据的统计分析。
使用Hive时需要注意，在将数据与表关联时，应该尽量使用外部表。只有在需要使用临时表时，才使用内部表。
另外，临时表在用完之后一定要删除，否则这些数据会留在hive里成为垃圾数据，越积越多，影响Hive正常的使用。

#### Spark
应该说Spark是大数据分析人员的最爱了。
RDD（Resilient Distributed Datasets，分布式弹性数据集）和DataFrame对数据的抽象，是对数据的一种矩阵表示，
因此对大数据分析人员而言，Spark天然就是为他们量身定制的工具。

Spark以RDD抽象为核心，提供了一系列的Transformation操作和Action操作API。
通过这些操作组合，可以实现复杂的计算模式和分析功能。
另外Spark充分使用内存来进行操作计算，相比Hadoop最初的MapReduce，会大幅提高程序性能。

### 调度
离线任务通常是周期性定时执行的，因此通常我们需要一个能够管理离线任务执行的调度系统。
比较简单的调度系统就是linux下的定时执行工具cron。
但是cron工具是一个功能简单的工具，只支持本地调度，也没有用户友好的管理界面。
当调度任务很多时，cron任务难以管理，任务执行状态也不方便追踪。
因此，我们需要引入更加强大的调度工具，比如Azkaban。

#### Azkaban
Azkaban是由LinkedIn为调度Hadoop作业而开发的批处理工作流调度器。
Azkaban解决了作业之间的依赖顺序问题，并提供了一个简单易用的Web用户界面来管理和追踪工作流。

Azkaban具有以下功能特点：

* 兼容任意版本的Hadoop
* 简单易用的Web UI
* 可以基于Web或Http方便上传工作流
* 支持项目空间
* 支持工作流调度
* 模块化和可插拔的插件机制
* 支持认证和授权
* 支持用户行为跟踪
* 支持作业执行状态邮件通知
* 支持SLA(服务等级协议)和作业自动杀死
* 支持失败作业重试

除了Azkaban外，还有些其它的工作流调度器，比如Oozie，Airflow等。这里就不再展开了，读者可以自行查阅相关资料。
